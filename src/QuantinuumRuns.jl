#!/usr/licensed/julia/1.11/bin/julia
"""
Module for processing shots from Quantinuum System H2,
multitree state preparation with added erasures or heralded coherent errors
"""
module QuantinuumRuns

using Nemo
using BellPrepSyndromeDecoder
using StatsBase
using BinaryFields
using Base.Threads

export process_shots_fail, process_shots, process_shots_coherent, process_fail, process_fail_samples

"""
take in data generated by process_shots
filter_out: exclude certain error rates
return: `fail_data` is dictionary of logical failure probability mean and average at different added error rates, averaged over different hardware runs and sampling in post-processing
"""
function process_fail_samples(processed_data; filter_out = [])
    rates = filter(el->!(el in filter_out), sort(collect(keys(processed_data["flips"]))))
    fail_data = Dict("p"=>[r[1] for r in rates], "nshots"=>zeros(Int,length(rates)), "r"=>zeros(length(rates)), "means"=>Dict("fail"=>zeros(length(rates))), 
            "stds"=>Dict("fail"=>zeros(length(rates)), "sample-fail"=>[]))
    for (r_i,r)=enumerate(rates)
        n_shots = length(processed_data["counts"][r])
	n_samples = sum(processed_data["counts"][r][1])
	rates = [sum([processed_data["counts"][r][i][2-processed_data["flips"][r][i][j],j] for j=1:size(processed_data["counts"][r][i],2)])/n_samples for i=1:n_shots]
	total_rate = mean(rates)
        fail_data["means"]["fail"][r_i] = 1-total_rate
        fail_data["stds"]["fail"][r_i] = std(rates)/sqrt(n_shots)
	push!(fail_data["stds"]["sample-fail"], [sqrt(ps * (1-ps)) for ps in rates] ./ sqrt(n_samples))
        n = sum(length.(vcat(processed_data["erasures"][r][1][1:2]...)))
        fail_data["r"][r_i] = mean([sum(sum.(vcat(processed_data["erasures"][r][i]...)))/n 
            for i=1:length(processed_data["erasures"][r])])
	fail_data["nshots"][r_i] = n_shots
    end
    fail_data
end

"""
take in data generated by process_shots_coherent
filter_out: exclude certain error rates
return: `fail_data` is dictionary of logical failure probability mean and average at different added error rates, averaged over different hardware runs. One sample per run, unlike process_fail_samples
"""
function process_fail(processed_data; filter_out = [])
    rates = filter(el->!(el in filter_out), sort(collect(keys(processed_data["flips"]))))
    fail_data = Dict("p"=>[r[1] for r in rates], "r"=>zeros(length(rates)), "means"=>Dict("fail"=>zeros(length(rates))), 
            "stds"=>Dict("fail"=>zeros(length(rates))))
    for (r_i,r)=enumerate(rates)
        n_shots = sum(processed_data["counts"][r])
        total_rate = sum([processed_data["counts"][r][2-processed_data["flips"][r][i],i] 
                for i=1:length(processed_data["flips"][r])])/n_shots
        std_rate = sqrt(total_rate * (1-total_rate))/sqrt(n_shots)
        fail_data["means"]["fail"][r_i] = 1-total_rate
        fail_data["stds"]["fail"][r_i] = std_rate
        n = sum(length.(vcat(processed_data["heralded"][r][1][1:2]...)))
        fail_data["r"][r_i] = sum([sum(sum.(vcat(processed_data["heralded"][r][i][1:2]...)))/n * sum(
                    processed_data["counts"][r][:,i]) 
            for i=1:length(processed_data["heralded"][r])])/n_shots
    end
    fail_data
end

"""
Master function for averaging over shots / samples, at different system sizes 
`depths` = system sizes T
"""
function process_shots_fail(processed_data, depths; n_qub=x->2^x, filter_out = [], f = process_fail_samples)
    fail_data = Dict()
    for t=depths
        fail_data[n_qub(t)] = f(processed_data[t]; filter_out = filter_out)
    end
    fail_data
end


"""
At a specified error rate, sample erasure patterns in post-processing for each shot and run stacked probability passing decoder
"""
function decode_partial(dat, bell_params, err_probs, erasure_probs, match_idxs, sys_idxs; num_samples::Int = 1)
    tmax = length(match_idxs)
    n_shots = size(dat,1)

    erasure_patterns = Array{Array}(undef, n_shots)
    erasure_patterns_post = Array{Array}(undef, n_shots)
    syndromes = Array{Array}(undef, n_shots)
    flip_bits = Array{Array}(undef, n_shots)
    myweights = Array{Array}(undef, n_shots)
    count_arr = Array{Array}(undef, n_shots)
    er_counts = zeros(n_shots)
    
    @threads for i=1:n_shots
        syndromes[i], erasure_patterns_post[i], erasure_patterns[i], er_counts[i], count_arr[i], myweights[i], flip_bits[i] = decode_partial_sample(dat[i,:], bell_params, err_probs, erasure_probs, match_idxs, sys_idxs; num_samples = num_samples, print = false)
    end

    total_rates =[sum([count_arr[i][2-flip_bits[i][j],j] for j=1:size(count_arr[i],2)])/num_samples for i=1:length(count_arr)]
    total_rate = mean(total_rates)
    std_rate = std(total_rates)/sqrt(n_shots)
    println("Successes: $(total_rate)\\pm$(std_rate)")
    return syndromes, erasure_patterns, erasure_patterns_post, count_arr, myweights, flip_bits, [1-total_rate, std_rate, mean(er_counts)]
end

"""
At a specified error rate, given raw data `dat` for a single shot, sample `num_samples` erasure patterns and run decoder
"""
function decode_partial_sample(dat, bell_params, err_probs, erasure_probs, match_idxs, sys_idxs; num_samples::Int = 1, print::Bool = false)
    tmax = length(match_idxs)
    erasure_pattern, outcomes = convert_outcomes(dat,tmax)
    er = vcat(vcat(erasure_pattern...)...)
    er_count = sum(er)/length(er)
    erasures_post = Array{Array}(undef, num_samples)
    log_bits = zeros(Bool, num_samples)
    all_syndromes = Array{Array}(undef, num_samples)

    @threads for j=1:num_samples
        erasures_post[j], samp_outcomes = add_measurement_erasures(erasure_probs, deepcopy(outcomes))
        all_syndromes[j], log_bits[j] = process_syndrome(samp_outcomes, bell_params.final_params.par_checks, match_idxs, sys_idxs, bell_params.check_times)
    end

    uniq_syndromes = Dict()
    for j=1:num_samples
        key = [erasures_post[j], all_syndromes[j]]
        if haskey(uniq_syndromes, key)
            if log_bits[j]
                uniq_syndromes[key] .+= [0,1]
            else
                uniq_syndromes[key] .+= [1,0]
            end
        elseif log_bits[j]
            uniq_syndromes[key] = [0,1]
        else
           uniq_syndromes[key] = [1,0]
        end
    end
    syndromes = sort(collect(keys(uniq_syndromes)), by=x->-sum(uniq_syndromes[x]))
    mycount_arr = hcat([uniq_syndromes[syndr] for syndr in syndromes]...)
    if print
    	println("Most likely syndrome: $(syndromes[1][2]), $(mycount_arr[:,1])")
    	println("Number of syndromes: $(length(syndromes)). Syndrome bits: $(length(vcat(vcat(vcat(syndromes[1][1]...)...)...)))")
    end
    
    myweights = zeros(2, length(syndromes))
    classes = zeros(Bool, length(syndromes))
    flip_bits = zeros(Bool, length(syndromes))

    @threads for i=1:length(syndromes)
        error_probs = initialize_error_prob_stacks(deepcopy(err_probs), vcat(erasure_pattern, [syndromes[i][1]]), 
            sys_idxs, match_idxs)
        myweights[:,i], classes[i] = level_bell_syndrome_decoder(bell_params, error_probs, syndromes[i][2])
        flip_bits[i] = ((argmax(myweights[:,i])==1)!=classes[i])
    end

    return [[[[nemo_to_bool(dat)[:,1] for dat in data] for data in datas] for datas in datass[2]] for datass in syndromes], 
         [syndr[1] for syndr in syndromes], erasure_pattern, er_count, mycount_arr, myweights, flip_bits
end

"""
Read in shot data from hardware runs
Resample num_samples samples per shot (added erasures on measurements / gates immediately before measurements)
Run stacked probability passing  decoder
"""
function process_shots(shots, log_state, tmax, base_probs; processed_data = Dict("syndromes"=>Dict(), "weights"=>Dict(), "flips"=>Dict(), "counts"=>Dict(),
        "erasures"=>Dict(), "post-erasures"=>Dict()), num_samples::Int = 1)
    bell_params = BellPrepParams(tmax; log_state = log_state, measure_first = log_state, alternate = true)
    counts = vcat([lp.counts for lp in bell_params.level_params], 
            [bell_params.final_params.level_params.counts]);
    match_idxs, sys_idxs = prepare_check_idxs(tmax);
    err_probs = prep_syndrome_decoder(counts, base_probs)

    rates = sort(collect(keys(shots)))
    fail_data = Dict("p"=>[r[1] for r in rates], "r"=>zeros(length(rates)), "means"=>Dict("fail"=>zeros(length(rates))), 
        "stds"=>Dict("fail"=>zeros(length(rates))))
    for (r_i,r)=enumerate(rates)
        processed_data["syndromes"][r], processed_data["erasures"][r], processed_data["post-erasures"][r], processed_data["counts"][r], 
        processed_data["weights"][r], processed_data["flips"][r], fail = decode_partial(
            shots[r], bell_params, err_probs, r, match_idxs, sys_idxs; num_samples = num_samples)
	println(r); flush(stdout)
        fail_data["means"]["fail"][r_i] = fail[1]
        fail_data["stds"]["fail"][r_i] = fail[2]
        fail_data["r"][r_i] = fail[3]
    end
    processed_data, fail_data
end

"""
Run stacked probability passing decoder for all shots at a given error rate, and heralded coherent errors with sin^2(theta/2) = p_err
"""
function decode_coherent(dat, bell_params, err_probs, match_idxs, sys_idxs; print::Bool = true, p_err = sin(pi/8)^2)
    n_shots = size(dat, 1)
    tmax = length(match_idxs)
    herald_patterns = Array{Array}(undef, n_shots)

    log_bits = zeros(Bool, n_shots)
    all_syndromes = Array{Array}(undef, n_shots)
    
    @threads for j=1:n_shots
        herald_patterns[j], outcomes = convert_outcomes_coherent(dat[j,:],tmax)
        all_syndromes[j], log_bits[j] = process_syndrome(outcomes, bell_params.final_params.par_checks, match_idxs, sys_idxs, bell_params.check_times)
    end
    
    uniq_syndromes = Dict()
    for j=1:n_shots
        key = [herald_patterns[j], all_syndromes[j]]
        if haskey(uniq_syndromes, key)
            if log_bits[j]
                uniq_syndromes[key] .+= [0,1]
            else
                uniq_syndromes[key] .+= [1,0]
            end
        elseif log_bits[j]
            uniq_syndromes[key] = [0,1]
        else
           uniq_syndromes[key] = [1,0]
        end
    end
    syndromes = sort(collect(keys(uniq_syndromes)), by=x->-sum(uniq_syndromes[x]))
    mycount_arr = hcat([uniq_syndromes[syndr] for syndr in syndromes]...)
    num_locations = length(vcat(vcat(syndromes[1][1]...)...))
    er_count = sum([sum(vcat(vcat(syndromes[j][1]...)...))*sum(mycount_arr[:,j]) for j=1:length(syndromes)])/(num_locations * n_shots)
    
    myweights = zeros(2, length(syndromes))
    classes = zeros(Bool, length(syndromes))
    flip_bits = zeros(Bool, length(syndromes))

    @threads for i=1:length(syndromes)
        error_probs = initialize_error_prob_stacks(deepcopy(err_probs), syndromes[i][1], 
            sys_idxs, match_idxs; p_err = p_err)
        myweights[:,i], classes[i] = level_bell_syndrome_decoder(bell_params, error_probs, syndromes[i][2])
        flip_bits[i] = ((argmax(myweights[:,i])==1)!=classes[i])
    end

    total_rate =sum([mycount_arr[2-flip_bits[j],j] for j=1:size(mycount_arr,2)])/n_shots

    std_rate = sqrt(total_rate * (1-total_rate))/sqrt(n_shots)

    if print
        println(er_count)
	println("Successes: $(total_rate)\\pm$(std_rate)")
    	println("Most likely syndrome: $(syndromes[1][2]), $(mycount_arr[:,1])")
    	println("Number of syndromes: $(length(syndromes)). Syndrome bits: $(length(vcat(vcat(vcat(syndromes[1][2]...)...)...)))")
    end

    return [[[[nemo_to_bool(dat)[:,1] for dat in data] for data in datas] for datas in datass[2]] for datass in syndromes], 
         [syndr[1] for syndr in syndromes], mycount_arr, myweights, flip_bits, [1-total_rate, std_rate, er_count]
end

"""
Read in raw data from hardware shots with heralded coherent errors,
and run stacked probability passing decoder at each rate of added errors
Keys of shots are the heralded error rate
`base_probs` = native (unheralded) noise rate
"""
function process_shots_coherent(shots, log_state, tmax, base_probs; processed_data = Dict("syndromes"=>Dict(), "weights"=>Dict(), "flips"=>Dict(), "counts"=>Dict(),
        "heralded"=>Dict()), print::Bool = true, p_err = sin(pi/8)^2)
    bell_params = BellPrepParams(tmax; log_state = log_state, measure_first = log_state, alternate = true)
    counts = vcat([lp.counts for lp in bell_params.level_params], 
            [bell_params.final_params.level_params.counts]);
    match_idxs, sys_idxs = prepare_check_idxs(tmax);
    err_probs = prep_syndrome_decoder(counts, base_probs)

    rates = sort(collect(keys(shots)))
    fail_data = Dict("p"=>[r[1] for r in rates], "r"=>zeros(length(rates)), "means"=>Dict("fail"=>zeros(length(rates))), 
        "stds"=>Dict("fail"=>zeros(length(rates))))
    for (r_i,r)=enumerate(rates)
        processed_data["syndromes"][r], processed_data["heralded"][r], processed_data["counts"][r], 
        processed_data["weights"][r], processed_data["flips"][r], fail = decode_coherent(
            shots[r], bell_params, err_probs, match_idxs, sys_idxs; print = print, p_err = p_err)
	println(r); flush(stdout)
        fail_data["means"]["fail"][r_i] = fail[1]
        fail_data["stds"]["fail"][r_i] = fail[2]
        fail_data["r"][r_i] = fail[3]
    end
    processed_data, fail_data
end

end # module